{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaturvediajay/AngularjsRestapiDemo/blob/master/Taxi_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EVKto-YXpvG"
      },
      "source": [
        "## Solving OpenAI Gym Environment - (Taxi-v2)\n",
        "\n",
        "In this Python demo, we'll try solving the classic cab-driver problem. The purpose of this notebook is to show how to solve OpenAI Gym environments. We'll demonstrate Q-learning & SARSA on the Taxi environment.\n",
        "\n",
        "Let's now look at the problem statement\n",
        "\n",
        "Here, the objective is to pick up the passenger from one position and drop them off at another in minimum possible time. For this problem, we'll consider our environment to be a 5x5 grid. \n",
        "\n",
        "<img src=\"cab_problem.png\" style=\"width: 300px;\">\n",
        "Image source: https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
        "\n",
        "There are 4 locations (R, G, Y, B) marked in the image. And the task is to pick up the passenger from one of the four locations and drop him off at other. There is a reward of +20 for a successful dropoff, and -1 for every timestep it takes and -10 for illegal pick-up and drop-off actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CZWGVshQXpvI"
      },
      "outputs": [],
      "source": [
        "# Import routines\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9S0ESMCXpvK"
      },
      "source": [
        "### Calling the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eXUTcOMfXpvK",
        "outputId": "25703a2e-e535-4662-cf93-91d242939ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "DeprecatedEnv",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Taxi-v2'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDeprecatedEnv\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e972a2cc0f0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Taxi-v2\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Create environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# helps in visualizing the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    127\u001b[0m                              if env_name == valid_env_spec._env_name]\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeprecatedEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Env {} not found (valid versions include {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnregisteredEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No registered env with id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDeprecatedEnv\u001b[0m: Env Taxi-v2 not found (valid versions include ['Taxi-v3'])"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Taxi-v2\") # Create environment\n",
        "\n",
        "state = env.reset()\n",
        "env.render()  # helps in visualizing the environment\n",
        "\n",
        "print(\"current state is :\" ,state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOxELWU5XpvM"
      },
      "source": [
        "#### Rendering:\n",
        "    - yellow: taxi is unoccupied\n",
        "    - green: taxi is occupied by a passenger\n",
        "    - blue: passenger\n",
        "    - magenta: destination\n",
        "    - other grids: locations\n",
        "    \n",
        "That means, for now, I'm at yellow box and need to pick the passenger from 'G' and drop him off at 'B'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxfts7zzXpvN"
      },
      "source": [
        "### State Space\n",
        "\n",
        "The state vector for this problem is (col_index, row_index, destination_locations, passenger_position)\n",
        "There are 5 rows, 5 columns and 4 destination locations. What about the passenger locations? 4 or 5?\n",
        "\n",
        "If the passenger is not in cab that means he could be only at one of the four locations. But we also need to account for 1 addition state if the passenger is inside the cab. So, passenger could be at any 4+1 possible locations.\n",
        "\n",
        "Therefore, the state space = 5x5x4x5 = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y-5btkKXpvN"
      },
      "outputs": [],
      "source": [
        "# Number of possible states\n",
        "state_size = env.observation_space.n \n",
        "print(\"State space : \", state_size)\n",
        "#print(\"Current state : \" ,env.env.s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQxfRCruXpvO"
      },
      "source": [
        "### Action Space\n",
        "\n",
        "At any state, the cab driver can either move in any of the four directions or it can pickup/ drop (legally or illegally)\n",
        "\n",
        "    - 0: south\n",
        "    - 1: north\n",
        "    - 2: east\n",
        "    - 3: west\n",
        "    - 4: pickup\n",
        "    - 5: drop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USsvl3-_XpvO"
      },
      "outputs": [],
      "source": [
        "# Number of possible actions\n",
        "action_size = env.action_space.n \n",
        "print(\"Action space : \", action_size) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idOQ7lSjXpvP"
      },
      "source": [
        "### Training\n",
        "\n",
        "Let's know solve the given MDP using Q-learning & SARSA.\n",
        "\n",
        "### Q-Learning\n",
        "Q-Learning is an off-policy optimal control algorithm. It learns the Q-values by taking the next action based on the greedy policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GzUjwXsXpvQ"
      },
      "outputs": [],
      "source": [
        "# Initialise q-table with zeros\n",
        "Q_table = np.zeros((state_size, action_size))\n",
        "print(Q_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_YQ0LRTXpvQ"
      },
      "outputs": [],
      "source": [
        "episodes = 100000        # Total episodes       \n",
        "\n",
        "#hyperparameters\n",
        "learning_rate = 0.1      # Learning rate\n",
        "gamma = 0.8              # discount factor\n",
        "epsilon = 0.1            # exploration -exploitation tradeoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUD6G-84XpvR"
      },
      "outputs": [],
      "source": [
        "# Keeping the policy epsilon-greedy\n",
        "def epsilon_greedy(state, table):\n",
        "    z = np.random.random() # Randomizes a number to select whether or not to expolit\n",
        "    \n",
        "    if z > epsilon:\n",
        "        action = np.argmax(table[state])    #Exploitation: this gets the action corresponding to max q-value of current state\n",
        "    else:\n",
        "        action = env.action_space.sample()    #Exploration: randomly choosing and action\n",
        "    \n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhmlm6bfXpvS"
      },
      "outputs": [],
      "source": [
        "start = time.time()    # tracking time\n",
        "deltas = []\n",
        "for episode in range(1,episodes+1):\n",
        "    \n",
        "    state = env.reset() # Reset the environment\n",
        "    done = False        # 'done' defines successfully dropping the passenger off; \n",
        "                        # resulting in an end of episode\n",
        "    step = 0\n",
        "    biggest_change = 0  # to keep a track of difference in the Q-values\n",
        "    \n",
        "    if episode % 5000 == 0:\n",
        "        print(\"Episode: {}\".format(episode))\n",
        "        \n",
        "    while not done:\n",
        "\n",
        "        action = epsilon_greedy(state, Q_table)\n",
        "        \n",
        "        # Take the action and observe the new state and reward\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        oldQ_table = Q_table[state, action]\n",
        "                \n",
        "        # UPDATE RULE\n",
        "        Q_table[state, action] += learning_rate * (reward + gamma * \n",
        "                                                   np.max(Q_table[new_state,:])-Q_table[state,action])\n",
        "        \n",
        "        biggest_change = max(biggest_change, np.abs(Q_table[state][action] - oldQ_table))\n",
        "        \n",
        "        state = new_state\n",
        "                             \n",
        "    deltas.append(biggest_change)\n",
        "    \n",
        "    if deltas[-1] < 0.00000001:\n",
        "        break\n",
        "        \n",
        "    episode += 1\n",
        "\n",
        "    \n",
        "end = time.time()\n",
        "training_time = end - start\n",
        "print(\"Time taken in seconds: \", training_time)\n",
        "print(\"maximum difference: \", deltas[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQcCzK3fXpvS"
      },
      "outputs": [],
      "source": [
        "Q_table[454]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snh7BQGzXpvT"
      },
      "source": [
        "#### Testing the Q-Table\n",
        "\n",
        "Let's know test our Q-learning agent on a different environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4RLHKR-XpvU"
      },
      "outputs": [],
      "source": [
        "# Let's change the environment\n",
        "state = env.reset()  # reset will set the environment to a new and random state\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx6iO79DXpvU"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "done = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "while(done == False):\n",
        "    \n",
        "    best_action = np.argmax(Q_table[state,:]) # selecting the best action basis Q-table\n",
        "    \n",
        "    # Take the best action and observe the new state and reward\n",
        "    state, reward, done, info = env.step(best_action) \n",
        "    cumulative_reward += reward  \n",
        "    \n",
        "    time.sleep(0.5)\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    print('Episode Reward = ', cumulative_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-WYX4F1XpvW"
      },
      "source": [
        "### SARSA\n",
        "\n",
        "SARSA is on-policy learning algorithm. Unlinke Q-learning, it learns the Q-values by taking the next action based on the current policy rather than a greedy policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fwRRykMXpvW"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "env.render()  # helps in visualizing the environment\n",
        "\n",
        "print(\"current state is :\" ,state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYhMuRl5XpvX"
      },
      "outputs": [],
      "source": [
        "# Initialise sarsa-table with zeros\n",
        "Sarsa_table = np.zeros((state_size, action_size))\n",
        "print(Sarsa_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftor1qB3XpvZ"
      },
      "outputs": [],
      "source": [
        "episodes = 800000\n",
        "start = time.time()    # tracking time\n",
        "deltas = []\n",
        "for episode in range(1,episodes+1):\n",
        "    \n",
        "    state = env.reset() # Reset the environment\n",
        "    done = False        # 'done' defines successfully dropping the passenger off; \n",
        "                        # resulting in an end of episode\n",
        "    step = 0\n",
        "    biggest_change = 0  # to keep track of difference in Q-values\n",
        "    \n",
        "    if episode % 10000 == 0:\n",
        "        print(\"Episode: {}\".format(episode))\n",
        "        \n",
        "    while not done:\n",
        "\n",
        "        action = epsilon_greedy(state, Sarsa_table)\n",
        "        \n",
        "        # Take the action and observe the new state and reward\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        # Get the action basis epsilon greedy policy\n",
        "        next_action = epsilon_greedy(next_state, Sarsa_table)\n",
        "        \n",
        "        oldSarsa_table = Sarsa_table[state, action]\n",
        "        \n",
        "        # UPDATE RULE\n",
        "        Sarsa_table[state, action] += learning_rate * (reward + gamma * Sarsa_table[next_state,next_action] \n",
        "                                                       -Sarsa_table[state,action])\n",
        "        \n",
        "        biggest_change = max(biggest_change, np.abs(Sarsa_table[state][action] \n",
        "                                                    - oldSarsa_table))\n",
        "        \n",
        "        state = new_state\n",
        "                             \n",
        "    deltas.append(biggest_change)\n",
        "    \n",
        "    if deltas[-1] < 0.00000001:\n",
        "        break\n",
        "        \n",
        "    episode += 1\n",
        "\n",
        "    \n",
        "end = time.time()\n",
        "training_time = end - start\n",
        "print(\"Time taken in seconds: \", training_time)\n",
        "print(\"maximum difference: \", deltas[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSxSzffuXpvZ"
      },
      "outputs": [],
      "source": [
        "Sarsa_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A6oBRE3Xpva"
      },
      "outputs": [],
      "source": [
        "Sarsa_table[263]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svRGfTwlXpva"
      },
      "source": [
        "#### Testing the SARSA table\n",
        "\n",
        "Let's know test our SARSA agent on a different environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1Bn7_3GXpvb"
      },
      "outputs": [],
      "source": [
        "# Let's change the environment\n",
        "state = env.reset()  # reset will set the environment to a new and random state\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKycbfIrXpvb"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "done = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "while(done == False):\n",
        "    \n",
        "    best_action = np.argmax(Sarsa_table[state,:]) # selecting the best action basis Sarsa-table\n",
        "    \n",
        "    # Take the best action and observe the new state and reward\n",
        "    state, reward, done, info = env.step(best_action) \n",
        "    cumulative_reward += reward  \n",
        "    \n",
        "    time.sleep(0.5)\n",
        "    clear_output(wait=True)\n",
        "    env.render()\n",
        "    print('Episode Reward = ', cumulative_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAIEcyLWXpvb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Taxi-v2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}